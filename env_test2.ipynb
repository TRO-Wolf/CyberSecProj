{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import collections\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MainCode.env import NetworkEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NetworkEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 2., 3., ..., 0., 0., 0.],\n",
       "        [1., 2., 3., ..., 0., 0., 0.],\n",
       "        [1., 2., 3., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 2., 3., ..., 0., 0., 0.],\n",
       "        [1., 2., 3., ..., 0., 0., 0.],\n",
       "        [1., 2., 3., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 26.0,\n",
       " False,\n",
       " True,\n",
       " {'reward': 1.0000000000000002})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, EPS_START, EPS_END, EPS_DECAY, grand_length, training_rate=0.8):\n",
    "\n",
    "\n",
    "        self.epsilon_start = EPS_START\n",
    "        self.epsilon_end = EPS_END\n",
    "        \n",
    "        self.epsilon = EPS_START\n",
    "        self.grand_length = grand_length\n",
    "        self.epsilon_decay_steps = self.grand_length - 2\n",
    "        self.epsilon_decay = (self.epsilon_start - self.epsilon_end) / int(self.epsilon_decay_steps * training_rate)\n",
    "        self.base_ep_steps = int(self.epsilon_decay_steps * training_rate)\n",
    "\n",
    "        self.total_steps = 0\n",
    "    \n",
    "    def adjust_epsilon(self):\n",
    "        \n",
    "        if self.total_steps < self.base_ep_steps:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = 0.01\n",
    "        \n",
    "        self.total_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 2000\n",
    "max_episode_steps = 20\n",
    "\n",
    "training_rate = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNConv1D(nn.Module):\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQNConv1D, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 1600, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(1600, 1600, 5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 3200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3200, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 3200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3200, actions_n)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        val = self.fc_val(conv_out)\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        return val + adv - adv.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1553, 17)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = 32\n",
    "\n",
    "#BATCH_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#BATCH_SIZE = 512\n",
    "\n",
    "#BATCH_SIZE = 6000\n",
    "\n",
    "#BATCH_SIZE = 8000\n",
    "\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 100\n",
    "#TAU = 0.0005\n",
    "#TAU = 0.005\n",
    "\n",
    "TAU = 100\n",
    "\n",
    "#LR = 0.0001\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = env.simulator.shape[-1]\n",
    "\n",
    "policy_net = DQNConv1D(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQNConv1D(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "# policy_net = CNN().to(device)\n",
    "# target_net = CNN().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_number = 2444\n",
    "\n",
    "# policy_net.load_state_dict(torch.load(f'special_{file_number}_net.pt'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# optimizer.load_state_dict(torch.load(f'special_opt_{file_number}_net.pt'))\n",
    "\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "agent = Agent(EPS_START=EPS_START, EPS_END=EPS_END,\n",
    "              EPS_DECAY=EPS_DECAY, grand_length=max_episodes, training_rate=training_rate)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    if np.random.rand() <= agent.epsilon:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "def select_action_his(state):\n",
    "    global steps_done\n",
    "    global eps_threshold\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNConv1D(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv1d(1553, 1600, kernel_size=(5,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(1600, 1600, kernel_size=(5,), stride=(1,))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc_val): Sequential(\n",
       "    (0): Linear(in_features=14400, out_features=3200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3200, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc_adv): Sequential(\n",
       "    (0): Linear(in_features=14400, out_features=3200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3200, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    #print(batch.action)\n",
    "    #print(type(batch.action))\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    expected_state_action_values = expected_state_action_values.to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Huber loss\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome(episode, reward, epsilon, total):\n",
    "    template = '{:>4d} | {}  | Reward: {:>8.3f} ({:>7.3f}) | '\n",
    "    template += 'Epsilon | {:.3f}'\n",
    "\n",
    "    print(template.format(episode, format_time(total),\n",
    "                          np.mean(reward[-50:]),\n",
    "                          np.mean(reward[-10:]),\n",
    "                          epsilon\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, info = env.reset()\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(memory.memory) < BATCH_SIZE:\n",
    "\n",
    "  \n",
    "    state,info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = select_action(state)\n",
    "        \n",
    "        #observation, reward, terminated, _ = env.step(action.item())\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_final_next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         [1., 2., 3.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1553, 17])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1553, 17])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = policy_net(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -28.5887],\n",
       "        [ -17.3646],\n",
       "        [ -80.1468],\n",
       "        [  61.0742],\n",
       "        [ -88.1253],\n",
       "        [-123.8924],\n",
       "        [ -20.8172],\n",
       "        [  70.1777],\n",
       "        [ 113.0910],\n",
       "        [ -43.3375],\n",
       "        [ 124.5359],\n",
       "        [ -84.0670],\n",
       "        [ 111.3903],\n",
       "        [-105.4474],\n",
       "        [ -12.7002],\n",
       "        [ -41.7212]], device='cuda:0', grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = select_action(state)\n",
    "        \n",
    "observation, reward, terminated, truncated, _ = env.step(action.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(memory.memory) < BATCH_SIZE:\n",
    "\n",
    "  \n",
    "    state,info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = select_action(state)\n",
    "        \n",
    "        #observation, reward, terminated, _ = env.step(action.item())\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10 | 00:00:06  | Reward:   -0.350 ( -0.350) | Epsilon | 0.983\n",
      "  20 | 00:00:11  | Reward:   -0.388 ( -0.425) | Epsilon | 0.976\n",
      "  30 | 00:00:16  | Reward:   -3.717 (-10.375) | Epsilon | 0.969\n",
      "  40 | 00:00:22  | Reward:   -2.856 ( -0.275) | Epsilon | 0.962\n",
      "  50 | 00:00:28  | Reward:   -2.337 ( -0.260) | Epsilon | 0.955\n",
      "  60 | 00:00:33  | Reward:   -4.354 (-10.435) | Epsilon | 0.948\n",
      "  70 | 00:00:38  | Reward:   -6.383 (-10.570) | Epsilon | 0.941\n",
      "  80 | 00:00:43  | Reward:   -4.435 ( -0.635) | Epsilon | 0.934\n",
      "  90 | 00:00:48  | Reward:   -4.486 ( -0.530) | Epsilon | 0.927\n",
      " 100 | 00:00:54  | Reward:   -4.513 ( -0.395) | Epsilon | 0.920\n",
      " 110 | 00:00:59  | Reward:   -2.541 ( -0.575) | Epsilon | 0.913\n",
      " 120 | 00:01:05  | Reward:   -0.587 ( -0.800) | Epsilon | 0.906\n",
      " 130 | 00:01:10  | Reward:   -0.569 ( -0.545) | Epsilon | 0.899\n",
      " 140 | 00:01:15  | Reward:   -2.574 (-10.555) | Epsilon | 0.892\n",
      " 150 | 00:01:21  | Reward:   -2.568 ( -0.365) | Epsilon | 0.885\n",
      " 160 | 00:01:27  | Reward:   -2.565 ( -0.560) | Epsilon | 0.878\n",
      " 170 | 00:01:32  | Reward:   -2.541 ( -0.680) | Epsilon | 0.871\n",
      " 180 | 00:01:39  | Reward:   -2.517 ( -0.425) | Epsilon | 0.864\n",
      " 190 | 00:01:45  | Reward:   -0.476 ( -0.350) | Epsilon | 0.857\n",
      " 200 | 00:01:51  | Reward:   -2.493 (-10.450) | Epsilon | 0.850\n",
      " 210 | 00:01:57  | Reward:   -2.508 ( -0.635) | Epsilon | 0.843\n",
      " 220 | 00:02:04  | Reward:   -2.508 ( -0.680) | Epsilon | 0.836\n",
      " 230 | 00:02:10  | Reward:   -4.459 (-10.180) | Epsilon | 0.829\n",
      " 240 | 00:02:15  | Reward:   -4.510 ( -0.605) | Epsilon | 0.822\n",
      " 250 | 00:02:21  | Reward:   -2.484 ( -0.320) | Epsilon | 0.815\n",
      " 260 | 00:02:27  | Reward:   -2.406 ( -0.245) | Epsilon | 0.808\n",
      " 270 | 00:02:33  | Reward:   -2.271 ( -0.005) | Epsilon | 0.801\n",
      " 280 | 00:02:37  | Reward:   -4.249 (-20.070) | Epsilon | 0.794\n",
      " 290 | 00:02:43  | Reward:   -4.159 ( -0.155) | Epsilon | 0.787\n",
      " 300 | 00:02:49  | Reward:   -4.075 (  0.100) | Epsilon | 0.780\n",
      " 310 | 00:02:55  | Reward:   -4.036 ( -0.050) | Epsilon | 0.773\n",
      " 320 | 00:03:01  | Reward:   -6.119 (-10.420) | Epsilon | 0.766\n",
      " 330 | 00:03:06  | Reward:   -2.142 ( -0.185) | Epsilon | 0.759\n",
      " 340 | 00:03:12  | Reward:   -1.597 (  2.570) | Epsilon | 0.752\n",
      " 350 | 00:03:17  | Reward:   -1.708 ( -0.455) | Epsilon | 0.745\n",
      " 360 | 00:03:23  | Reward:   -3.704 (-10.030) | Epsilon | 0.738\n",
      " 370 | 00:03:29  | Reward:   -1.732 ( -0.560) | Epsilon | 0.731\n",
      " 380 | 00:03:36  | Reward:   -3.761 (-10.330) | Epsilon | 0.724\n",
      " 390 | 00:03:42  | Reward:   -4.306 ( -0.155) | Epsilon | 0.717\n",
      " 400 | 00:03:48  | Reward:   -6.224 (-10.045) | Epsilon | 0.710\n",
      " 410 | 00:03:54  | Reward:   -6.248 (-10.150) | Epsilon | 0.703\n",
      " 420 | 00:03:60  | Reward:   -8.232 (-10.480) | Epsilon | 0.696\n",
      " 430 | 00:04:06  | Reward:   -8.268 (-10.510) | Epsilon | 0.689\n",
      " 440 | 00:04:12  | Reward:   -8.361 ( -0.620) | Epsilon | 0.682\n",
      " 450 | 00:04:18  | Reward:   -8.427 (-10.375) | Epsilon | 0.675\n",
      " 460 | 00:04:23  | Reward:   -6.515 ( -0.590) | Epsilon | 0.668\n",
      " 470 | 00:04:28  | Reward:   -4.519 ( -0.500) | Epsilon | 0.661\n",
      " 480 | 00:04:33  | Reward:   -2.505 ( -0.440) | Epsilon | 0.654\n",
      " 490 | 00:04:39  | Reward:   -2.445 ( -0.320) | Epsilon | 0.647\n",
      " 500 | 00:04:45  | Reward:   -2.442 (-10.360) | Epsilon | 0.639\n",
      " 510 | 00:04:51  | Reward:   -2.412 ( -0.440) | Epsilon | 0.632\n",
      " 520 | 00:04:56  | Reward:   -4.390 (-10.390) | Epsilon | 0.625\n",
      " 530 | 00:05:01  | Reward:   -4.393 ( -0.455) | Epsilon | 0.618\n",
      " 540 | 00:05:06  | Reward:   -6.395 (-10.330) | Epsilon | 0.611\n",
      " 550 | 00:05:12  | Reward:   -4.399 ( -0.380) | Epsilon | 0.604\n",
      " 560 | 00:05:17  | Reward:   -4.399 ( -0.440) | Epsilon | 0.597\n",
      " 570 | 00:05:23  | Reward:   -2.415 ( -0.470) | Epsilon | 0.590\n",
      " 580 | 00:05:28  | Reward:   -2.457 ( -0.665) | Epsilon | 0.583\n",
      " 590 | 00:05:33  | Reward:   -0.482 ( -0.455) | Epsilon | 0.576\n",
      " 600 | 00:05:39  | Reward:   -0.485 ( -0.395) | Epsilon | 0.569\n",
      " 610 | 00:05:44  | Reward:   -0.479 ( -0.410) | Epsilon | 0.562\n",
      " 620 | 00:05:49  | Reward:   -2.547 (-10.810) | Epsilon | 0.555\n",
      " 630 | 00:05:55  | Reward:   -2.445 ( -0.155) | Epsilon | 0.548\n",
      " 640 | 00:05:60  | Reward:   -3.911 ( -7.785) | Epsilon | 0.541\n",
      " 650 | 00:06:05  | Reward:   -5.925 (-10.465) | Epsilon | 0.534\n",
      " 660 | 00:06:10  | Reward:   -5.922 ( -0.395) | Epsilon | 0.527\n",
      " 670 | 00:06:16  | Reward:   -3.830 ( -0.350) | Epsilon | 0.520\n",
      " 680 | 00:06:21  | Reward:   -3.956 ( -0.785) | Epsilon | 0.513\n",
      " 690 | 00:06:26  | Reward:   -2.478 ( -0.395) | Epsilon | 0.506\n",
      " 700 | 00:06:31  | Reward:   -2.484 (-10.495) | Epsilon | 0.499\n",
      " 710 | 00:06:37  | Reward:   -2.373 (  0.160) | Epsilon | 0.492\n",
      " 720 | 00:06:42  | Reward:   -7.795 (-27.460) | Epsilon | 0.485\n",
      " 730 | 00:06:47  | Reward:   -7.660 ( -0.110) | Epsilon | 0.478\n",
      " 740 | 00:06:53  | Reward:   -7.555 (  0.130) | Epsilon | 0.471\n",
      " 750 | 00:06:58  | Reward:   -5.517 ( -0.305) | Epsilon | 0.464\n",
      " 760 | 00:07:03  | Reward:   -5.610 ( -0.305) | Epsilon | 0.457\n",
      " 770 | 00:07:09  | Reward:   -2.199 (-10.405) | Epsilon | 0.450\n",
      " 780 | 00:07:14  | Reward:   -2.205 ( -0.140) | Epsilon | 0.443\n",
      " 790 | 00:07:19  | Reward:   -2.403 ( -0.860) | Epsilon | 0.436\n",
      " 800 | 00:07:24  | Reward:   -4.513 (-10.855) | Epsilon | 0.429\n",
      " 810 | 00:07:30  | Reward:   -4.591 ( -0.695) | Epsilon | 0.422\n",
      " 820 | 00:07:35  | Reward:   -2.625 ( -0.575) | Epsilon | 0.415\n",
      " 830 | 00:07:41  | Reward:   -4.702 (-10.525) | Epsilon | 0.408\n",
      " 840 | 00:07:46  | Reward:   -4.693 ( -0.815) | Epsilon | 0.401\n",
      " 850 | 00:07:52  | Reward:   -2.604 ( -0.410) | Epsilon | 0.394\n",
      " 860 | 00:07:57  | Reward:   -2.511 ( -0.230) | Epsilon | 0.387\n",
      " 870 | 00:08:02  | Reward:   -6.473 (-20.385) | Epsilon | 0.380\n",
      " 880 | 00:08:07  | Reward:   -6.455 (-10.435) | Epsilon | 0.373\n",
      " 890 | 00:08:13  | Reward:   -6.497 ( -1.025) | Epsilon | 0.366\n",
      " 900 | 00:08:18  | Reward:   -7.927 ( -7.560) | Epsilon | 0.359\n",
      " 910 | 00:08:23  | Reward:   -9.932 (-10.255) | Epsilon | 0.352\n",
      " 920 | 00:08:28  | Reward:   -7.969 (-10.570) | Epsilon | 0.345\n",
      " 930 | 00:08:34  | Reward:   -5.967 ( -0.425) | Epsilon | 0.338\n",
      " 940 | 00:08:39  | Reward:   -5.751 (  0.055) | Epsilon | 0.331\n",
      " 950 | 00:08:44  | Reward:   -6.353 (-10.570) | Epsilon | 0.324\n",
      " 960 | 00:08:50  | Reward:   -6.353 (-10.255) | Epsilon | 0.317\n",
      " 970 | 00:08:55  | Reward:   -4.420 ( -0.905) | Epsilon | 0.310\n",
      " 980 | 00:09:01  | Reward:   -4.354 ( -0.095) | Epsilon | 0.303\n",
      " 990 | 00:09:06  | Reward:   -4.456 ( -0.455) | Epsilon | 0.296\n",
      "1000 | 00:09:12  | Reward:   -2.529 ( -0.935) | Epsilon | 0.289\n",
      "1010 | 00:09:17  | Reward:   -0.434 (  0.220) | Epsilon | 0.282\n",
      "1020 | 00:09:22  | Reward:   -1.181 ( -4.640) | Epsilon | 0.275\n",
      "1030 | 00:09:28  | Reward:   -1.139 (  0.115) | Epsilon | 0.268\n",
      "1040 | 00:09:33  | Reward:    0.083 (  5.655) | Epsilon | 0.261\n",
      "1050 | 00:09:39  | Reward:   -1.167 ( -7.185) | Epsilon | 0.254\n",
      "1060 | 00:09:44  | Reward:    0.432 (  8.215) | Epsilon | 0.247\n",
      "1070 | 00:09:50  | Reward:    2.509 (  5.745) | Epsilon | 0.240\n",
      "1080 | 00:09:55  | Reward:    1.112 ( -6.870) | Epsilon | 0.233\n",
      "1090 | 00:10:01  | Reward:   -1.387 ( -6.840) | Epsilon | 0.226\n",
      "1100 | 00:10:06  | Reward:    0.687 (  3.185) | Epsilon | 0.219\n",
      "1110 | 00:10:12  | Reward:    0.687 (  8.215) | Epsilon | 0.212\n",
      "1120 | 00:10:17  | Reward:   -1.321 ( -4.295) | Epsilon | 0.205\n",
      "1130 | 00:10:22  | Reward:    2.705 ( 13.260) | Epsilon | 0.198\n",
      "1140 | 00:10:28  | Reward:    5.737 (  8.320) | Epsilon | 0.191\n",
      "1150 | 00:10:33  | Reward:    5.755 (  3.275) | Epsilon | 0.184\n",
      "1160 | 00:10:39  | Reward:    5.767 (  8.275) | Epsilon | 0.177\n",
      "1170 | 00:10:44  | Reward:    8.787 ( 10.805) | Epsilon | 0.170\n",
      "1180 | 00:10:50  | Reward:    8.805 ( 13.350) | Epsilon | 0.163\n",
      "1190 | 00:10:55  | Reward:    4.994 (-10.735) | Epsilon | 0.156\n",
      "1200 | 00:11:00  | Reward:    2.830 ( -7.545) | Epsilon | 0.149\n",
      "1210 | 00:11:06  | Reward:    1.572 (  1.985) | Epsilon | 0.142\n",
      "1220 | 00:11:11  | Reward:   -0.605 ( -0.080) | Epsilon | 0.135\n",
      "1230 | 00:11:17  | Reward:   -5.356 (-10.405) | Epsilon | 0.128\n",
      "1240 | 00:11:23  | Reward:   -3.336 ( -0.635) | Epsilon | 0.121\n",
      "1250 | 00:11:28  | Reward:   -1.385 (  2.210) | Epsilon | 0.114\n",
      "1260 | 00:11:34  | Reward:   -1.858 ( -0.380) | Epsilon | 0.107\n",
      "1270 | 00:11:39  | Reward:   -3.941 (-10.495) | Epsilon | 0.100\n",
      "1280 | 00:11:44  | Reward:   -3.959 (-10.495) | Epsilon | 0.093\n",
      "1290 | 00:11:50  | Reward:   -3.896 ( -0.320) | Epsilon | 0.086\n",
      "1300 | 00:11:55  | Reward:   -6.464 (-10.630) | Epsilon | 0.079\n",
      "1310 | 00:11:60  | Reward:   -7.960 ( -7.860) | Epsilon | 0.072\n",
      "1320 | 00:12:05  | Reward:   -7.463 ( -8.010) | Epsilon | 0.065\n",
      "1330 | 00:12:11  | Reward:   -7.412 (-10.240) | Epsilon | 0.058\n",
      "1340 | 00:12:16  | Reward:   -5.906 (  7.210) | Epsilon | 0.051\n",
      "1350 | 00:12:22  | Reward:   -2.769 (  5.055) | Epsilon | 0.044\n",
      "1360 | 00:12:27  | Reward:    1.338 ( 12.675) | Epsilon | 0.037\n",
      "1370 | 00:12:33  | Reward:    2.912 ( -0.140) | Epsilon | 0.030\n",
      "1380 | 00:12:38  | Reward:    2.819 (-10.705) | Epsilon | 0.023\n",
      "1390 | 00:12:44  | Reward:    2.882 (  7.525) | Epsilon | 0.016\n",
      "1400 | 00:12:49  | Reward:    1.795 ( -0.380) | Epsilon | 0.010\n",
      "1410 | 00:12:55  | Reward:    0.822 (  7.810) | Epsilon | 0.010\n",
      "1420 | 00:13:00  | Reward:    1.298 (  2.240) | Epsilon | 0.010\n",
      "1430 | 00:13:06  | Reward:    3.854 (  2.075) | Epsilon | 0.010\n",
      "1440 | 00:13:12  | Reward:    0.780 ( -7.845) | Epsilon | 0.010\n",
      "1450 | 00:13:18  | Reward:    2.265 (  7.045) | Epsilon | 0.010\n",
      "1460 | 00:13:23  | Reward:    0.281 ( -2.110) | Epsilon | 0.010\n",
      "1470 | 00:13:29  | Reward:   -0.318 ( -0.755) | Epsilon | 0.010\n",
      "1480 | 00:13:35  | Reward:    0.781 (  7.570) | Epsilon | 0.010\n",
      "1490 | 00:13:41  | Reward:    3.879 (  7.645) | Epsilon | 0.010\n",
      "1500 | 00:13:46  | Reward:    3.981 (  7.555) | Epsilon | 0.010\n",
      "1510 | 00:13:52  | Reward:    5.378 (  4.875) | Epsilon | 0.010\n",
      "1520 | 00:13:57  | Reward:    5.005 ( -2.620) | Epsilon | 0.010\n",
      "1530 | 00:14:02  | Reward:    4.463 (  4.860) | Epsilon | 0.010\n",
      "1540 | 00:14:08  | Reward:    2.437 ( -2.485) | Epsilon | 0.010\n",
      "1550 | 00:14:13  | Reward:    2.943 ( 10.085) | Epsilon | 0.010\n",
      "1560 | 00:14:19  | Reward:    1.781 ( -0.935) | Epsilon | 0.010\n",
      "1570 | 00:14:25  | Reward:    4.280 (  9.875) | Epsilon | 0.010\n",
      "1580 | 00:14:30  | Reward:    6.484 ( 15.880) | Epsilon | 0.010\n",
      "1590 | 00:14:36  | Reward:    6.854 ( -0.635) | Epsilon | 0.010\n",
      "1600 | 00:14:41  | Reward:    6.402 (  7.825) | Epsilon | 0.010\n",
      "1610 | 00:14:47  | Reward:    8.049 (  7.300) | Epsilon | 0.010\n",
      "1620 | 00:14:53  | Reward:    6.453 (  1.895) | Epsilon | 0.010\n",
      "1630 | 00:14:58  | Reward:    2.828 ( -2.245) | Epsilon | 0.010\n",
      "1640 | 00:15:04  | Reward:    3.903 (  4.740) | Epsilon | 0.010\n",
      "1650 | 00:15:09  | Reward:    2.720 (  1.910) | Epsilon | 0.010\n",
      "1660 | 00:15:15  | Reward:    3.316 ( 10.280) | Epsilon | 0.010\n",
      "1670 | 00:15:21  | Reward:    4.499 (  7.810) | Epsilon | 0.010\n",
      "1680 | 00:15:26  | Reward:    4.920 ( -0.140) | Epsilon | 0.010\n",
      "1690 | 00:15:32  | Reward:    6.959 ( 14.935) | Epsilon | 0.010\n",
      "1700 | 00:15:37  | Reward:    4.981 ( -7.980) | Epsilon | 0.010\n",
      "1710 | 00:15:42  | Reward:    4.424 (  7.495) | Epsilon | 0.010\n",
      "1720 | 00:15:48  | Reward:    4.792 (  9.650) | Epsilon | 0.010\n",
      "1730 | 00:15:54  | Reward:    9.002 ( 20.910) | Epsilon | 0.010\n",
      "1740 | 00:15:59  | Reward:   10.173 ( 20.790) | Epsilon | 0.010\n",
      "1750 | 00:16:05  | Reward:   13.437 (  8.340) | Epsilon | 0.010\n",
      "1760 | 00:16:10  | Reward:   15.614 ( 18.380) | Epsilon | 0.010\n",
      "1770 | 00:16:16  | Reward:   14.313 (  3.145) | Epsilon | 0.010\n",
      "1780 | 00:16:21  | Reward:   15.331 ( 26.000) | Epsilon | 0.010\n",
      "1790 | 00:16:27  | Reward:   16.373 ( 26.000) | Epsilon | 0.010\n",
      "1800 | 00:16:32  | Reward:   16.882 ( 10.885) | Epsilon | 0.010\n",
      "1810 | 00:16:38  | Reward:   17.903 ( 23.485) | Epsilon | 0.010\n",
      "1820 | 00:16:43  | Reward:   19.451 ( 10.885) | Epsilon | 0.010\n",
      "1830 | 00:16:48  | Reward:   18.412 ( 20.805) | Epsilon | 0.010\n",
      "1840 | 00:16:54  | Reward:   15.220 ( 10.040) | Epsilon | 0.010\n",
      "1850 | 00:16:60  | Reward:   15.662 ( 13.095) | Epsilon | 0.010\n",
      "1860 | 00:17:05  | Reward:   13.645 ( 13.400) | Epsilon | 0.010\n",
      "1870 | 00:17:10  | Reward:   16.668 ( 26.000) | Epsilon | 0.010\n",
      "1880 | 00:17:16  | Reward:   17.204 ( 23.485) | Epsilon | 0.010\n",
      "1890 | 00:17:21  | Reward:   20.396 ( 26.000) | Epsilon | 0.010\n",
      "1900 | 00:17:27  | Reward:   19.394 (  8.085) | Epsilon | 0.010\n",
      "1910 | 00:17:33  | Reward:   20.363 ( 18.245) | Epsilon | 0.010\n",
      "1920 | 00:17:38  | Reward:   16.801 (  8.190) | Epsilon | 0.010\n",
      "1930 | 00:17:43  | Reward:   15.151 ( 15.235) | Epsilon | 0.010\n",
      "1940 | 00:17:49  | Reward:    8.373 ( -7.890) | Epsilon | 0.010\n",
      "1950 | 00:17:55  | Reward:    9.276 ( 12.600) | Epsilon | 0.010\n",
      "1960 | 00:18:00  | Reward:    7.707 ( 10.400) | Epsilon | 0.010\n",
      "1970 | 00:18:05  | Reward:    5.587 ( -2.410) | Epsilon | 0.010\n",
      "1980 | 00:18:10  | Reward:    2.197 ( -1.715) | Epsilon | 0.010\n",
      "1990 | 00:18:16  | Reward:    5.783 ( 10.040) | Epsilon | 0.010\n",
      "2000 | 00:18:21  | Reward:    3.299 (  0.180) | Epsilon | 0.010\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 20\n",
    "else:\n",
    "    num_episodes = 5\n",
    "\n",
    "total_rewards = []\n",
    "total = 0\n",
    "start = time()\n",
    "\n",
    "agent_reward = []\n",
    "\n",
    "\n",
    "#for i_episode in range(num_episodes):\n",
    "results = []\n",
    "\n",
    "for episode in range(1,max_episodes + 1):\n",
    "\n",
    "\n",
    "    state,info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if done:\n",
    "            total = reward.cpu().numpy()\n",
    "            next_state = None\n",
    "            total_rewards.append(reward.cpu().numpy())\n",
    "            episode_durations.append(episode_step + 1)\n",
    "            break\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "\n",
    "        \n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        if episode_step % TAU == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    agent_reward.append(total)\n",
    "\n",
    "    agent.adjust_epsilon()\n",
    "\n",
    "\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        outcome(episode, agent_reward, agent.epsilon, time() - start)\n",
    "        # value = round(total[0], 4)\n",
    "        # print(\n",
    "        #     f'reward: {value}'\n",
    "        # )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'policy_net.pt')\n",
    "torch.save(optimizer.state_dict(), 'optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
