{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6751818b",
   "metadata": {},
   "source": [
    "# Increasing the Notebook Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9235385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c96dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Native Python Libraries\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import collections\n",
    "import re\n",
    "from collections import namedtuple, deque, defaultdict\n",
    "from itertools import count\n",
    "\n",
    "# Importing Numpy and Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "\n",
    "# Importing Matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing TA-Lib\n",
    "import talib\n",
    "\n",
    "# Importing PyTorch Libraries\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Importing SciKit Learn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n",
    "\n",
    "# Importing Gymnasium Libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "# Importing TQDM to track cell progress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ignoring Warnings\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aef78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Importing Tensorflow Reinforcement Learning Libraries\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5106b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make sure to add .gitignore to temp_directory\n",
      "file already exist\n"
     ]
    }
   ],
   "source": [
    "# Importing Custom Code\n",
    "from MainCode.env import Simulator, Window, CustomTensorDataReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8bde7f",
   "metadata": {},
   "source": [
    "# Verifying Tensorflow Utilizes GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbf2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6497a0",
   "metadata": {},
   "source": [
    "# Initializing Custom Classes Built from Pre-processing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd543ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our class to read our numpy matrix files and merging them\n",
    "dr = CustomTensorDataReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc996fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our class to manage our environment Window\n",
    "# This is our login window (1 second time stamp)\n",
    "wd = Window()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac9bf",
   "metadata": {},
   "source": [
    "# Showing the Shape of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c86506e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62223, 1553, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 62223 Time Stamps\n",
    "# 1553 Logins for each time stamp (this includes padding entries with 0)\n",
    "# 17 items per login entry\n",
    "dr.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40186fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = wd.reset_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d241d0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1553, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d057e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.target_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e90aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49468"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.current_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e33a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62223, 1553, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how to access our data\n",
    "# This accesses the first login, represented by j, of our first time stamp, i\n",
    "# wd.data[i][j]\n",
    "wd.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45425c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14863575,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e4922",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a3c68415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Conversion\n",
    "# wd.target_data = tf.convert_to_tensor(wd.target_data)\n",
    "# wd.data = tf.convert_to_tensor(wd.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9446e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n"
     ]
    }
   ],
   "source": [
    "if (1 not in wd.target_data):\n",
    "    print('Not in')\n",
    "else:\n",
    "    print(\"In\")\n",
    "\n",
    "# if (np.any(wd.target_data, 1) == True):\n",
    "#     print('Got it')\n",
    "# else:\n",
    "#     print('damn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f31b9f",
   "metadata": {},
   "source": [
    "# Creating Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05a6ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take: We can Flag an item, Continue to the next item, or Finish/Hold once we have analyzed the current log/activity\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "        # Time Stamp Login Array\n",
    "        self.observation_space = Box(low=0, high=1552, shape=(1553,17), dtype=int)\n",
    "#         self.observation_space = Box(low= np.array([0]), high = np.array([1552]), shape=(1553,17), dtype=int)\n",
    "        \n",
    "        # Set Starting Login Entry\n",
    "        self.state = random.randint(0,1552)\n",
    "        \n",
    "        # Set Network Log Length\n",
    "        self.log_length = 1553\n",
    "        \n",
    "        # Set Network Health\n",
    "        self.network_health = 100\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply Actions\n",
    "        # -1 If we have run out of items in the list we want to declare that we are finished OR if we are in a live environment we want to hold until our next activity\n",
    "        # 0 If we flag the item we want to perform an action before we continue OR if we are holding we want to flag when we start\n",
    "        # 1 If we don't detect abnormal behavior we want to continue to process the next item in the list\n",
    "        self.state = action\n",
    "        \n",
    "        # Decreamenting the length of our log by 1\n",
    "        self.log_length -= 1 \n",
    "        \n",
    "        # Calculating Reward\n",
    "        # Cases which our agent is correct\n",
    "        if ((self.state == 1) and (1 in wd.target_window)) or ((self.state == 0) and (1 not in wd.target_window)):\n",
    "            reward = 1 \n",
    "        # Cases which our agent is incorrect    \n",
    "        else:\n",
    "            reward = -1\n",
    "            self.network_health -= 25\n",
    "            \n",
    "            \n",
    "        if self.log_length == 0 or self.network_health <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Placeholder for info\n",
    "        info = {}\n",
    "    \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset our environment\n",
    "        self.state = random.randint(0,1552)\n",
    "        # Reset our log length\n",
    "        self.log_length = 1553\n",
    "        # Reset our network health\n",
    "        self.network_health = 100\n",
    "        \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c5d77",
   "metadata": {},
   "source": [
    "# Setting env Variable to Custom Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f634e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NetworkEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec0d25dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 521,  134,  594, ...,  429, 1015,  895],\n",
       "       [ 249,  655, 1189, ..., 1228, 1144, 1300],\n",
       "       [1438, 1153, 1489, ...,  549,  582,  406],\n",
       "       ...,\n",
       "       [ 334, 1456,  395, ..., 1265,   70, 1545],\n",
       "       [1069,  202,   64, ...,  947,  987,  373],\n",
       "       [ 822,  834,  820, ...,  312,  731,  592]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224c40a",
   "metadata": {},
   "source": [
    "# Testing Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d7fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-4\n",
      "Episode:2 Score:0\n",
      "Episode:3 Score:-4\n",
      "Episode:4 Score:-2\n",
      "Episode:5 Score:-1\n",
      "Episode:6 Score:-3\n",
      "Episode:7 Score:-4\n",
      "Episode:8 Score:-1\n",
      "Episode:9 Score:-3\n",
      "Episode:10 Score:-2\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "#     network_health = 100\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "#         network_health += health\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682cd4a7",
   "metadata": {},
   "source": [
    "# Building Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62fb345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_dims):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), \n",
    "                                    dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85806173",
   "metadata": {},
   "source": [
    "# Building Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "253766c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate implementation to build model which adds layers on top\n",
    "def build_neuralnet(lr, states, actions, window):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(24, activation='relu', kernel_size=window, input_shape=states))\n",
    "    model.add(Conv1D(24, activation='relu', kernel_size=window))\n",
    "    model.add(Dense(actions, activation=None))\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f96f7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_neuralnet(states, actions, lr, window):\n",
    "#     model = Sequential([\n",
    "#         Conv1D(24, activation='relu', kernel_size=window, input_shape=states),\n",
    "#         Conv1D(24, activation='relu', kernel_size=window),\n",
    "#         Linear(actions, activation=None)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=lr), loss='mae')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b181c679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(1553, 17)\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(actions)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc046bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model # This is here to prevent issues after initial build, uncomment to rebuilding model\n",
    "model = build_neuralnet(3, states, actions, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03d6bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94397a8c",
   "metadata": {},
   "source": [
    "## Summary of Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6f9c2aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_11 (Conv1D)          (None, 1537, 24)          6960      \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 1521, 24)          9816      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1521, 3)           75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,851\n",
      "Trainable params: 16,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3702c",
   "metadata": {},
   "source": [
    "# Building Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c5f48679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent():\n",
    "    def __init__(self, lr, gamma, actions, epsilon, batch_size, input_dims, epsilon_dec=1e-3, epsilon_end=0.01, mem_size=50000, fname='dqn_model.h5', **kwargs):\n",
    "        \n",
    "        # Setting Hyperparameters\n",
    "        window = 17\n",
    "        self.action_space = [i for i in range(actions)] # Possible list of actions\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.model_file = fname\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
    "        self.q_eval = build_neuralnet(lr, actions, input_dims, window)\n",
    "        \n",
    "    # Storing our transitions\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    # Dictating how the Agent chooses an action, random vs maximal reward\n",
    "    def choose_action(self, observation):\n",
    "        # If we are in the exploratory phase choose a random action (in our case randomly provide a label to learn)\n",
    "        # else we make a predicition based off of what we have learned\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state = np.array([observation])\n",
    "            actions = self.q_eval.predict(state)     \n",
    "            \n",
    "            # Choosing the \"Greediest\" action\n",
    "            action = np.argmax(actions)\n",
    "            return action\n",
    "    \n",
    "    # Controlling Agent's learning\n",
    "    def learn(self):\n",
    "        # If we have not completed learning equal to a batch size, we want to continue\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        q_eval = self.q_eval.predict(states)\n",
    "        q_eval = self.q_eval.predict(states_)\n",
    "        \n",
    "        q_target = np.copy(q_eval)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        # Updating Q target value with actions the agent took\n",
    "        q_target[batch_index, actions] = rewards + self.gamma * np.max(q_next, axis=1)*dones\n",
    "        \n",
    "        self.q_eval.train_on_batch(states, q_target)\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "      \n",
    "    # Saving our model\n",
    "    def save_model(self):\n",
    "        self.q_eval.save(self.model_file)\n",
    "       \n",
    "    # Loading our model\n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c968fde",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      2\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 3\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m eps_history \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[95], line 13\u001b[0m, in \u001b[0;36mDQAgent.__init__\u001b[1;34m(self, lr, gamma, actions, epsilon, batch_size, input_dims, epsilon_dec, epsilon_end, mem_size, fname, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_file \u001b[38;5;241m=\u001b[39m fname\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m ReplayBuffer(mem_size, input_dims)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_eval \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_neuralnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[88], line 4\u001b[0m, in \u001b[0;36mbuild_neuralnet\u001b[1;34m(lr, states, actions, window)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_neuralnet\u001b[39m(lr, states, actions, window):\n\u001b[0;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 4\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Conv1D(\u001b[38;5;241m24\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39mwindow))\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Dense(actions, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layout_param_name, layout \u001b[38;5;129;01min\u001b[39;00m layout_args\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\layers\\convolutional\\conv1d.py:152\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mallow_initializer_layout\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    151\u001b[0m ):\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    153\u001b[0m         rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    154\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    155\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39mkernel_size,\n\u001b[0;32m    156\u001b[0m         strides\u001b[38;5;241m=\u001b[39mstrides,\n\u001b[0;32m    157\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    158\u001b[0m         data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[0;32m    159\u001b[0m         dilation_rate\u001b[38;5;241m=\u001b[39mdilation_rate,\n\u001b[0;32m    160\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    161\u001b[0m         activation\u001b[38;5;241m=\u001b[39mactivations\u001b[38;5;241m.\u001b[39mget(activation),\n\u001b[0;32m    162\u001b[0m         use_bias\u001b[38;5;241m=\u001b[39muse_bias,\n\u001b[0;32m    163\u001b[0m         kernel_initializer\u001b[38;5;241m=\u001b[39minitializers\u001b[38;5;241m.\u001b[39mget(kernel_initializer),\n\u001b[0;32m    164\u001b[0m         bias_initializer\u001b[38;5;241m=\u001b[39minitializers\u001b[38;5;241m.\u001b[39mget(bias_initializer),\n\u001b[0;32m    165\u001b[0m         kernel_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39mget(kernel_regularizer),\n\u001b[0;32m    166\u001b[0m         bias_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39mget(bias_regularizer),\n\u001b[0;32m    167\u001b[0m         activity_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39mget(activity_regularizer),\n\u001b[0;32m    168\u001b[0m         kernel_constraint\u001b[38;5;241m=\u001b[39mconstraints\u001b[38;5;241m.\u001b[39mget(kernel_constraint),\n\u001b[0;32m    169\u001b[0m         bias_constraint\u001b[38;5;241m=\u001b[39mconstraints\u001b[38;5;241m.\u001b[39mget(bias_constraint),\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    171\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py:118\u001b[0m, in \u001b[0;36mConv.__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, conv_op, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     96\u001b[0m     rank,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    117\u001b[0m ):\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    119\u001b[0m         trainable\u001b[38;5;241m=\u001b[39mtrainable,\n\u001b[0;32m    120\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m    121\u001b[0m         activity_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39mget(activity_regularizer),\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m=\u001b[39m rank\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filters, \u001b[38;5;28mfloat\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:447\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m             batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 447\u001b[0m         batch_input_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_input_shape \u001b[38;5;241m=\u001b[39m batch_input_shape\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Manage initial weight values if passed.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "lr = 0.001\n",
    "agent = DQAgent(lr=lr, gamma=0.99, actions=actions, epsilon=1.0, batch_size=64, input_dims=env.observation_space.shape)\n",
    "scores = []\n",
    "eps_history = []\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        observation = observation_\n",
    "        agent.learn()\n",
    "    eps_history.append(agent.epsilon)\n",
    "    scores.append(score)\n",
    "    \n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode: ', i, 'score %.2f' % score,\n",
    "         'average_score %.2f' % avg_score,\n",
    "         'epsilon %.2f' % agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fd879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0186e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
