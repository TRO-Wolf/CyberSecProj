{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import collections\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MainCode.env import NetworkEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NetworkEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, EPS_START, EPS_END, EPS_DECAY, grand_length, training_rate=0.8):\n",
    "\n",
    "\n",
    "        self.epsilon_start = EPS_START\n",
    "        self.epsilon_end = EPS_END\n",
    "        \n",
    "        self.epsilon = EPS_START\n",
    "        self.grand_length = grand_length\n",
    "        self.epsilon_decay_steps = self.grand_length - 2\n",
    "        self.epsilon_decay = (self.epsilon_start - self.epsilon_end) / int(self.epsilon_decay_steps * training_rate)\n",
    "        self.base_ep_steps = int(self.epsilon_decay_steps * training_rate)\n",
    "\n",
    "        self.total_steps = 0\n",
    "    \n",
    "    def adjust_epsilon(self):\n",
    "        \n",
    "        if self.total_steps < self.base_ep_steps:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = 0.01\n",
    "        \n",
    "        self.total_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 2000\n",
    "max_episode_steps = 20\n",
    "\n",
    "training_rate = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNConv1D(nn.Module):\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQNConv1D, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 1600, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(1600, 1600, 5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 3200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3200, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 3200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3200, actions_n)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        val = self.fc_val(conv_out)\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        return val + adv - adv.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = 32\n",
    "\n",
    "#BATCH_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#BATCH_SIZE = 512\n",
    "\n",
    "#BATCH_SIZE = 6000\n",
    "\n",
    "#BATCH_SIZE = 8000\n",
    "\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 100\n",
    "#TAU = 0.0005\n",
    "#TAU = 0.005\n",
    "\n",
    "TAU = 100\n",
    "\n",
    "#LR = 0.0001\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = env.simulator.shape[-1]\n",
    "\n",
    "policy_net = DQNConv1D(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQNConv1D(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "# policy_net = CNN().to(device)\n",
    "# target_net = CNN().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_number = 2444\n",
    "\n",
    "# policy_net.load_state_dict(torch.load(f'special_{file_number}_net.pt'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# optimizer.load_state_dict(torch.load(f'special_opt_{file_number}_net.pt'))\n",
    "\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "agent = Agent(EPS_START=EPS_START, EPS_END=EPS_END,\n",
    "              EPS_DECAY=EPS_DECAY, grand_length=max_episodes, training_rate=training_rate)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    if np.random.rand() <= agent.epsilon:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "def select_action_his(state):\n",
    "    global steps_done\n",
    "    global eps_threshold\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    #print(batch.action)\n",
    "    #print(type(batch.action))\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    expected_state_action_values = expected_state_action_values.to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Huber loss\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome(episode, reward, epsilon, total):\n",
    "    template = '{:>4d} | {}  | Reward: {:>8.3f} ({:>7.3f}) | '\n",
    "    template += 'Epsilon | {:.3f}'\n",
    "\n",
    "    print(template.format(episode, format_time(total),\n",
    "                          np.mean(reward[-50:]),\n",
    "                          np.mean(reward[-10:]),\n",
    "                          epsilon\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(memory.memory) < BATCH_SIZE:\n",
    "\n",
    "  \n",
    "    state,info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = select_action(state)\n",
    "        \n",
    "        #observation, reward, terminated, _ = env.step(action.item())\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 20\n",
    "else:\n",
    "    num_episodes = 5\n",
    "\n",
    "total_rewards = []\n",
    "total = 0\n",
    "start = time()\n",
    "\n",
    "agent_reward = []\n",
    "\n",
    "\n",
    "#for i_episode in range(num_episodes):\n",
    "results = []\n",
    "\n",
    "for episode in range(1,max_episodes + 1):\n",
    "\n",
    "\n",
    "    state,info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if done:\n",
    "            total = reward.cpu().numpy()\n",
    "            next_state = None\n",
    "            total_rewards.append(reward.cpu().numpy())\n",
    "            episode_durations.append(episode_step + 1)\n",
    "            break\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "\n",
    "        \n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        if episode_step % TAU == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    agent_reward.append(total)\n",
    "\n",
    "    agent.adjust_epsilon()\n",
    "\n",
    "\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        outcome(episode, agent_reward, agent.epsilon, time() - start)\n",
    "        # value = round(total[0], 4)\n",
    "        # print(\n",
    "        #     f'reward: {value}'\n",
    "        # )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
